{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "69e7471b",
      "metadata": {
        "id": "69e7471b"
      },
      "source": [
        "# Exam on Convolutional Neural Networks (CNN)\n",
        "\n",
        "Welcome to the Convolutional Neural Networks (CNN) practical exam. In this exam, you will work on an image classification task to predict the type of traffic sign. You are provided with a dataset of traffic sign images, and your task is to build, train, and evaluate a CNN model.\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset Overview\n",
        "### **Dataset:**\n",
        "* Just run the command under the `Load Data` section to get the data downloaded and unzipped or you can access it [here](https://drive.google.com/file/d/1HwMV-Lt_sWoxc5v6igmTxTwomS3DR6cQ/view?usp=sharing)\n",
        "### **Dataset Name:** Traffic Signs\n",
        "\n",
        "### **Description:**  \n",
        "The dataset contains images of various German traffic signs labeled for classification purposes. Each image belongs to one of the 43 classes, representing different types of traffic signs.\n",
        "\n",
        "### **Labels:**\n",
        "```python\n",
        "classes = {\n",
        "    0:  'Speed limit (20km/h)',\n",
        "    1:  'Speed limit (30km/h)',\n",
        "    2:  'Speed limit (50km/h)',\n",
        "    3:  'Speed limit (60km/h)',\n",
        "    4:  'Speed limit (70km/h)',\n",
        "    5:  'Speed limit (80km/h)',\n",
        "    6:  'End of speed limit (80km/h)',\n",
        "    7:  'Speed limit (100km/h)',\n",
        "    8:  'Speed limit (120km/h)',\n",
        "    9:  'No passing',\n",
        "    10: 'No passing veh over 3.5 tons',\n",
        "    11: 'Right-of-way at intersection',\n",
        "    12: 'Priority road',\n",
        "    13: 'Yield',\n",
        "    14: 'Stop',\n",
        "    15: 'No vehicles',\n",
        "    16: 'Veh > 3.5 tons prohibited',\n",
        "    17: 'No entry',\n",
        "    18: 'General caution',\n",
        "    19: 'Dangerous curve left',\n",
        "    20: 'Dangerous curve right',\n",
        "    21: 'Double curve',\n",
        "    22: 'Bumpy road',\n",
        "    23: 'Slippery road',\n",
        "    24: 'Road narrows on the right',\n",
        "    25: 'Road work',\n",
        "    26: 'Traffic signals',\n",
        "    27: 'Pedestrians',\n",
        "    28: 'Children crossing',\n",
        "    29: 'Bicycles crossing',\n",
        "    30: 'Beware of ice/snow',\n",
        "    31: 'Wild animals crossing',\n",
        "    32: 'End speed + passing limits',\n",
        "    33: 'Turn right ahead',\n",
        "    34: 'Turn left ahead',\n",
        "    35: 'Ahead only',\n",
        "    36: 'Go straight or right',\n",
        "    37: 'Go straight or left',\n",
        "    38: 'Keep right',\n",
        "    39: 'Keep left',\n",
        "    40: 'Roundabout mandatory',\n",
        "    41: 'End of no passing',\n",
        "    42: 'End no passing veh > 3.5 tons'\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66c6f645",
      "metadata": {
        "id": "66c6f645"
      },
      "source": [
        "## Load Data\n",
        "Run the following command to get the data and unzip it, alternatively you can access the data [here](https://drive.google.com/file/d/1HwMV-Lt_sWoxc5v6igmTxTwomS3DR6cQ/view?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "ee08de6b",
      "metadata": {
        "id": "ee08de6b"
      },
      "outputs": [],
      "source": [
        "# #https://drive.google.com/file/d/1HwMV-Lt_sWoxc5v6igmTxTwomS3DR6cQ/view?usp=sharing\n",
        "# !pip install gdown\n",
        "# !gdown --id 1HwMV-Lt_sWoxc5v6igmTxTwomS3DR6cQ\n",
        "# !unzip Traffic_Signs.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d khaledzsa/traffic-signs\n",
        "!unzip -q traffic-signs.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gPuFjULnysr",
        "outputId": "8ef868e3-659d-451f-edce-76e067413f99"
      },
      "id": "9gPuFjULnysr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/khaledzsa/traffic-signs\n",
            "License(s): unknown\n",
            "traffic-signs.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "replace Test_Dataset/Test_Dataset/Dataset/00000.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bb9f30e",
      "metadata": {
        "id": "9bb9f30e"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22207f85",
      "metadata": {
        "id": "22207f85"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3aa9945d",
      "metadata": {
        "id": "3aa9945d"
      },
      "source": [
        "## Data Preprocessing\n",
        "In this section, preprocess the dataset by:\n",
        "- Loading the images from the file paths.\n",
        "- Resizing the images to a consistent size.\n",
        "- Normalizing pixel values.\n",
        "\n",
        "Add more if needed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72042d3e",
      "metadata": {
        "id": "72042d3e"
      },
      "outputs": [],
      "source": [
        "img_height = 224\n",
        "img_width = 224\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "num_classes = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "541f3c2b",
      "metadata": {
        "id": "541f3c2b"
      },
      "source": [
        "## Data Splitting\n",
        "In this section, we will split our dataset into three parts:\n",
        "\n",
        "* Training set (70%).\n",
        "* Validation set (15%).\n",
        "* Test set (15%)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define paths to your dataset folders\n",
        "train_data_dir = '/content/Traffic_Signs/Traffic_Signs/Dataset'\n",
        "test_data_dir = '/content/Test_Dataset/Test_Dataset/Dataset'\n",
        "\n",
        "# Define constants\n",
        "classes = 43  # Number of classes in the dataset\n",
        "img_size = 32  # Standardize image size to 32x32 pixels\n",
        "\n",
        "# Load and preprocess the training data\n",
        "train_data = []\n",
        "train_labels = []\n",
        "\n",
        "for i in range(classes):\n",
        "    path = os.path.join(train_data_dir, str(i))\n",
        "    for img in os.listdir(path):\n",
        "        try:\n",
        "            img_array = cv2.imread(os.path.join(path, img))\n",
        "            resized_img = cv2.resize(img_array, (img_size, img_size))\n",
        "            train_data.append(resized_img)\n",
        "            train_labels.append(i)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img}: {e}\")"
      ],
      "metadata": {
        "id": "ND0kTr42FDI3"
      },
      "id": "ND0kTr42FDI3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Convert to numpy arrays and normalize\n",
        "train_data = np.array(train_data)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "train_data = train_data / 255.0  # Normalize the image data to [0, 1] range\n",
        "train_labels = to_categorical(train_labels, classes)  # One-hot encode the labels\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "datagen_train = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n"
      ],
      "metadata": {
        "id": "l_YCknwVF474"
      },
      "id": "l_YCknwVF474",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://drive.google.com/file/d/1S_vpQntND9839x8kJpegaEgtSIA4JxHO/view?usp=sharing\n",
        "!gdown --id 1S_vpQntND9839x8kJpegaEgtSIA4JxHO\n",
        "!unzip Test_Dataset.zip"
      ],
      "metadata": {
        "id": "8-oPae7OI3HA"
      },
      "id": "8-oPae7OI3HA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is only for debugging purposes\n",
        "datagen_val = ImageDataGenerator()\n",
        "\n",
        "test_data = []\n",
        "test_labels = []\n",
        "\n",
        "# Looping through all available folders in test data directory\n",
        "for img in os.listdir(test_data_dir):\n",
        "    img_path = os.path.join(test_data_dir, img)\n",
        "    try:\n",
        "        img_array = cv2.imread(img_path)\n",
        "        resized_img = cv2.resize(img_array, (img_size, img_size))\n",
        "        test_data.append(resized_img)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {img}: {e}\")\n",
        "\n",
        "\n",
        "if len(test_data) > 0:\n",
        "    test_data = np.array(test_data) / 255.0  # Normalize the image data\n",
        "    test_labels = to_categorical(test_labels, classes)  # One-hot encode the labels\n",
        "else:\n",
        "    print(\"No test data found!\")\n",
        "\n",
        "# Verify if data was loaded correctly\n",
        "print(f\"Loaded {len(test_data)} test samples.\")\n",
        "\n",
        "# Convert to numpy arrays and normalize\n",
        "test_data = np.array(test_data)"
      ],
      "metadata": {
        "id": "CV56nJsFHmJG"
      },
      "id": "CV56nJsFHmJG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Il9vBoZqrZfd"
      },
      "id": "Il9vBoZqrZfd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y3OTxV69Tqie"
      },
      "id": "Y3OTxV69Tqie",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4kz5Z8VpG17v"
      },
      "id": "4kz5Z8VpG17v"
    },
    {
      "cell_type": "markdown",
      "id": "83974596",
      "metadata": {
        "id": "83974596"
      },
      "source": [
        "## Building the CNN Model\n",
        "In this section, define the architecture of the CNN model. The architecture may consist of:\n",
        "- Convolutional layers with max-pooling\n",
        "- Dropout layers\n",
        "- Flatten layer\n",
        "- Dense layers\n",
        "- Output layer\n",
        "\n",
        "Add and remove any of these as needed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "977da65a",
      "metadata": {
        "id": "977da65a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Build the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(classes, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "fgQluAIXHZpz"
      },
      "id": "fgQluAIXHZpz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "MzWtJAfcID4I"
      },
      "id": "MzWtJAfcID4I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fe1c0940",
      "metadata": {
        "id": "fe1c0940"
      },
      "source": [
        "## Training the Model\n",
        "Train the CNN model using the training data and validate it on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce31faf9",
      "metadata": {
        "id": "ce31faf9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Train the model with data augmentation\n",
        "history = model.fit(\n",
        "    datagen_train.flow(X_train, y_train, batch_size=32),\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25ad1b22",
      "metadata": {
        "id": "25ad1b22"
      },
      "source": [
        "## Evaluate the Model\n",
        "Evaluate the performance of the model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access and print training and validation accuracy\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "# Access and print training and validation loss\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Print the metrics\n",
        "print(\"Epoch-wise Performance Metrics:\\n\")\n",
        "\n",
        "print(f\"  Train Accuracy: {train_accuracy[-1]}\")\n",
        "print(f\"  Validation Accuracy: {val_accuracy[-1]}\")\n",
        "print(f\"  Train Loss: {train_loss[-1]}\")\n",
        "print(f\"  Validation Loss: {val_loss[-1]}\")\n",
        "print()\n"
      ],
      "metadata": {
        "id": "wLXiBx_WV8zZ"
      },
      "id": "wLXiBx_WV8zZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_data)\n",
        "predicted_classes = np.argmax(predictions, axis=1)"
      ],
      "metadata": {
        "id": "R1zH5vJosOQH"
      },
      "id": "R1zH5vJosOQH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a37fca33",
      "metadata": {
        "id": "a37fca33"
      },
      "source": [
        "\n",
        "## Make Predictions\n",
        "Use the trained model to make predictions on new or unseen traffic sign images."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predicted classes for the test images:\")\n",
        "for i in range(len(predicted_classes)):\n",
        "    print(f\"Image {i+1}: Predicted class index: {predicted_classes[i]}\")\n"
      ],
      "metadata": {
        "id": "XfdFVODnsVK9"
      },
      "id": "XfdFVODnsVK9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f1ab3608",
      "metadata": {
        "id": "f1ab3608"
      },
      "source": [
        "if you need new, we prepared some data for you [here](https://drive.google.com/file/d/1S_vpQntND9839x8kJpegaEgtSIA4JxHO/view?usp=sharing), or you can simply run the following command to get the data and unzip it.\n",
        "\n",
        "<small>Note: please note that the file contain MetaData to tell you what each image contains <b>THIS IS JUST FOR YOU TO MAKE SURE</b></smmall>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8430600d",
      "metadata": {
        "id": "8430600d"
      },
      "source": [
        "\n",
        "## Model Performance Visualization\n",
        "Visualize performance metrics such as accuracy and loss over the epochs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualize Performance Metrics (Accuracy and Loss) Over the Epochs\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j89C60PdPGRL"
      },
      "id": "j89C60PdPGRL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8d4f3348",
      "metadata": {
        "id": "8d4f3348"
      },
      "source": [
        "\n",
        "## Save the Model\n",
        "Save the trained CNN model for submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe6b2df0",
      "metadata": {
        "id": "fe6b2df0"
      },
      "outputs": [],
      "source": [
        "# Save the Trained CNN Model\n",
        "model_save_path = \"traffic_signs_cnn_model.h5\"\n",
        "model.save(model_save_path)\n",
        "print(f\"Model saved at {model_save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed90995f",
      "metadata": {
        "id": "ed90995f"
      },
      "source": [
        "## Project Questions:\n",
        "\n",
        "\n",
        "1. **Data Preprocessing**: Explain why you chose your specific data preprocessing techniques (e.g., resizing images, normalization, data augmentation). How do these preprocessing steps improve the performance of your CNN model?\n",
        "2. **Model Architecture**: Describe the architecture of your CNN model (e.g., number of convolutional layers, kernel sizes, pooling layers). Why did you choose this structure, and how do you expect each layer to contribute to feature extraction?\n",
        "3. **Activation Functions**: Justify your choice of activation functions. How do they influence the training and output of your CNN?\n",
        "4. **Training Process**: Discuss your choice of batch size, number of epochs, and optimizer. How did these decisions impact the training process and the convergence of the model?\n",
        "5. **Loss Function and Metrics**: Explain why you chose the specific loss function and evaluation metrics for this classification task. How do they align with the goal of correctly classifying traffic signs?\n",
        "6. **Regularization Techniques**: If you used regularization methods like dropout or batch normalization, explain why you implemented them and how they helped prevent overfitting in your model.\n",
        "7. **Model Evaluation**: Justify the method you used to evaluate your model's performance on the test set. Why did you select these evaluation techniques, and what insights did they provide about your model's accuracy and generalization ability?\n",
        "8. **Model Visualization**: Explain the significance of the performance visualizations (e.g., accuracy and loss curves). What do they tell you about your model's training process and its ability to generalize?\n",
        "9. **Overfitting and Underfitting**: Analyze whether the model encountered any overfitting or underfitting during training. What strategies could you implement to mitigate these issues?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02e7f114",
      "metadata": {
        "id": "02e7f114"
      },
      "source": [
        "### Answer Here:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JwZg6C7PJ5-u"
      },
      "id": "JwZg6C7PJ5-u"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Questions:\n",
        "\n",
        "1. **Data Preprocessing**:  \n",
        "   Resizing images ensures a consistent input size for the CNN, while normalization scales pixel values, making the model converge faster during training. Data augmentation introduces variability e.g., rotations, flips to prevent overfitting and enhance generalization.\n",
        "\n",
        "2. **Model Architecture**:  \n",
        "   The model consists of multiple convolutional layers with small kernels 3x3, followed by layers for dimensionality reduction. This structure captures spatial hierarchies effectively. Deeper layers extract more features crucial for classification.\n",
        "\n",
        "3. **Activation Functions**:  \n",
        "   ReLU was chosen for hidden layers due to its ability to avoid the gradient problem, leading to faster convergence. Softmax is used in the output layer for multi-class classification, ensuring a probability distribution across classes.\n",
        "\n",
        "4. **Training Process**:  \n",
        "   A batch size of 32 and 30 epochs were selected to balance training time and model performance. The Adam optimizer was chosen for its adaptive learning rate, helping achieve faster convergence with fewer tuning requirements.\n",
        "\n",
        "5. **Loss Function and Metrics**:  \n",
        "   The categorical cross-entropy loss function is ideal for multi-class classification tasks. Accuracy was used as the primary metric since the goal is to correctly classify traffic signs into distinct categories.\n",
        "\n",
        "6. **Regularization Techniques**:  \n",
        "   Dropout was implemented to prevent overfitting by randomly deactivating neurons during training, ensuring the model does not rely too heavily on specific features. This enhances the model's ability to generalize to unseen data.\n",
        "\n",
        "7. **Model Evaluation**:  \n",
        "   The model was evaluated using the test set with metrics like accuracy and loss. These metrics provide a clear indication of the model's performance and generalization ability, highlighting areas where the model may struggle.\n",
        "\n",
        "8. **Model Visualization**:  \n",
        "   Accuracy and loss curves help in monitoring the training process. A significant gap between training and validation performance would indicate overfitting, while converging curves suggest a well-generalized model.\n",
        "\n",
        "9. **Overfitting and Underfitting**:  \n",
        "   If overfitting was observed, data augmentation and dropout were strategies used to counteract it. Early stopping could also be employed to halt training when validation loss starts to increase, preventing overfitting further.\n",
        "\n"
      ],
      "metadata": {
        "id": "WivdAoqEYR25"
      },
      "id": "WivdAoqEYR25"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gCuTpxk4Yo5R"
      },
      "id": "gCuTpxk4Yo5R",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}